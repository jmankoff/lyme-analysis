{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations\n",
    "\n",
    "This notebook works better if you run\n",
    "\n",
    "```conda install -c conda-forge jupyter_contrib_nbextensions``` \n",
    "\n",
    "on your command line.\n",
    "and find the configuration screen (available at the bottom of your ```Edit``` menu)\n",
    "Then turn on ```codefolding```, ```table of contents```, and ```initialization cells```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.core.display import HTML\n",
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Loads the helpers notebook, and the config files\n",
    "%run Helpers.ipynb\n",
    "%run LoadData.ipynb\n",
    "hello_world()\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "with open(\"userconfig.json\", 'r') as file_obj:\n",
    "    config = json.load(file_obj)\n",
    "#     file_obj.close()\n",
    "filedir = config[\"configdir\"]\n",
    "surveydir = config[\"surveydir\"]\n",
    "\n",
    "# Load the json file that has the data loading and cleaning info in it. \n",
    "# this is currently v1 and that can change if we make tweaks to things \n",
    "# like the cutoff for non-response\n",
    "with open(filedir + \"dataconfig.json\", 'r') as file_obj:\n",
    "    dataconfig = json.load(file_obj)\n",
    "#     file_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# loads the surveys and cleans them\n",
    "clean_surveys = load_surveys(config, dataconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clean_surveys:\n",
    "    clean_surveys[i].to_csv(surveydir + i + \"_cleaned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data to do new merging\n",
    "clean_surveys = {}\n",
    "for i in ['uw_baseline1', 'uw_baseline2', 'uw_mid', 'uw_post', 'uw_discrimination', 'cmuII_baseline1', 'cmuII_baseline2', 'cmuII_post']:\n",
    "    clean_surveys[i] = pd.read_csv(surveydir + i + \"_cleaned.csv\")\n",
    "    clean_surveys[i].set_index(\"PID\",inplace = True)\n",
    "    clean_surveys[i][\"PID\"] = clean_surveys[i].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "scaled_surveys = create_scales(config, dataconfig, copy.deepcopy(clean_surveys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in scaled_surveys:\n",
    "    scaled_surveys[i].to_csv(surveydir + i + \"_with_scale.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data to do new merging\n",
    "scaled_surveys = {}\n",
    "for i in ['uw_baseline1', 'uw_baseline2', 'uw_mid', 'uw_post', 'uw_discrimination', 'cmuII_baseline1', 'cmuII_baseline2', 'cmuII_post']:\n",
    "    scaled_surveys[i] = pd.read_csv(surveydir + i + \"_with_scale.csv\")\n",
    "    scaled_surveys[i].set_index(\"PID\",inplace = True)\n",
    "    scaled_surveys[i][\"PID\"] = scaled_surveys[i].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge into a single dataframe by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the baseline files by rows (allowing null columns if one column appears in one survey but not in another)\n",
    "\n",
    "configdir = config[\"configdir\"]\n",
    "with open(configdir+dataconfig[\"mergeconfig\"], 'r') as file_obj:\n",
    "    print(configdir)\n",
    "    mergeconfig = json.load(file_obj)\n",
    "    file_obj.close()\n",
    "with open(configdir+dataconfig[\"scaleconfig\"], 'r') as file_obj:\n",
    "    scaleconfig = json.load(file_obj)\n",
    "    file_obj.close()\n",
    "\n",
    "merged, merged_uw, merged_cmu = merge_surveys_by_row(config, dataconfig, mergeconfig, copy.deepcopy(scaled_surveys))\n",
    "merged.to_csv(surveydir+\"merged_by_row_all.csv\")\n",
    "merged_uw.to_csv(surveydir+\"merged_by_row_uw.csv\")\n",
    "merged_cmu.to_csv(surveydir+\"merged_by_row_cmu.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged into a single dataframe by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Create an exportable or analyzable summary file that merges baseline 1 and baseline 3. \n",
    "\n",
    "\n",
    "merged = merge_surveys_by_column(config, dataconfig, mergeconfig, scaled_surveys)\n",
    "display(HTML(merged.head(n=10).to_html()))\n",
    "merged.index.name = \"PID\"\n",
    "merged = merge_scales(config, dataconfig, merged)\n",
    "merged.to_csv(surveydir+dataconfig[\"mergedfile\"])\n",
    "\n",
    "# # XX I had this note: default for discrimination columns should be nan => 0\n",
    "# # XX but I am currently setting it to -1. \n",
    "# print(\"TODO: determine whether discrimination nans need to be set to 0 and how to do that\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Merged Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the helpers notebook, and the config files\n",
    "#%run Helpers.ipynb\n",
    "#%run LoadData.ipynb\n",
    "\n",
    "with open(\"userconfig.json\", 'r') as file_obj:\n",
    "    config = json.load(file_obj)\n",
    "    file_obj.close()\n",
    "filedir = config[\"configdir\"]\n",
    "\n",
    "# Load the json files that have the data loading and cleaning info.\n",
    "# this is currently v1 and that can change if we make tweaks to things \n",
    "# like the cutoff for non-response\n",
    "with open(filedir+config[\"dataconfig\"], 'r') as file_obj:\n",
    "    dataconfig = json.load(file_obj)\n",
    "    file_obj.close()\n",
    "with open(filedir+dataconfig[\"mergeconfig\"], 'r') as file_obj:\n",
    "    mergeconfig = json.load(file_obj)\n",
    "    file_obj.close()\n",
    "        \n",
    "file = config[\"surveydir\"]+dataconfig[\"mergedfile\"]\n",
    "merged = pd.read_csv(file) \n",
    "\n",
    "display(HTML(merged.head(5).to_html()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate population specific data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run LoadData.ipynb\n",
    "# Load the json file that has the data loading and cleaning info in it. \n",
    "# this is currently v1 and that can change if we make tweaks to things \n",
    "# like the cutoff for non-response\n",
    "with open(filedir + \"dataconfig.json\", 'r') as file_obj:\n",
    "    dataconfig = json.load(file_obj)\n",
    "\n",
    "\n",
    "#merged_cmu = copy.deepcopy(merged)\n",
    "merged_cmu = merged[merged.LOC_ALL == 1]\n",
    "\n",
    "#merged_uw = copy.deepcopy(merged)\n",
    "merged_uw = merged[merged.LOC_ALL == 0]\n",
    "\n",
    "df_all = merged\n",
    "df_cmu = merged_cmu\n",
    "df_uw = merged_uw\n",
    "\n",
    "print(\"------------generating datasets for all\")\n",
    "datasets = generate_datasets(dataconfig, df_all)\n",
    "print(\"------------generating datasets for CMU\")\n",
    "cmu_datasets = generate_datasets(dataconfig, df_cmu)\n",
    "print(\"------------generating datasets for UW\")\n",
    "uw_datasets = generate_datasets(dataconfig, df_uw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact List Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PID - contact type - contact num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_type = [\"Contacts_2\", \"Contacts_3\", \"Contacts_4\", \"Contacts_5\", \"Contacts_6\", \"Contacts_7\", \"Contacts_8\", \"Contacts_9\", \"Contacts_10\", \"Contacts_11\", \"Contacts_12\", \"Contacts_13\", \"Contacts_14\", \"Contacts_15\", \"Contacts_16\", \"Contacts_17\", \"Contacts_18\", \"Contacts_19\", \"Contacts_20\", \"Contacts_21\"]\n",
    "contact_num = [\"Contacts_2_TEXT\", \"Contacts_3_TEXT\", \"Contacts_4_TEXT\", \"Contacts_5_TEXT\", \"Contacts_6_TEXT\", \"Contacts_7_TEXT\", \"Contacts_8_TEXT\", \"Contacts_9_TEXT\", \"Contacts_10_TEXT\", \"Contacts_11_TEXT\", \"Contacts_12_TEXT\", \"Contacts_13_TEXT\", \"Contacts_14_TEXT\", \"Contacts_15_TEXT\", \"Contacts_16_TEXT\", \"Contacts_17_TEXT\", \"Contacts_18_TEXT\", \"Contacts_19_TEXT\", \"Contacts_20_TEXT\", \"Contacts_21_TEXT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_map = {1.0:\"Family\", 2.0:\"Local Friend\", 3.0: \"Out-of-town Friend\", 11.0 : \"Other\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_list_pre = []\n",
    "\n",
    "a = clean_surveys[\"uw_baseline2\"][contact_type]\n",
    "b = clean_surveys[\"uw_baseline2\"][contact_num]\n",
    "for row_index, row in enumerate(a.iterrows()):\n",
    "    l = row[1]\n",
    "    for col_index,t in enumerate(l):\n",
    "        if (not pd.isna(t)):\n",
    "            n_ori = b.iloc[row_index, col_index]\n",
    "            n = str(n_ori).replace(\" \",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"-\",\"\").replace(\"+\",\"\")\n",
    "            if (n == \"\"): continue\n",
    "            contact_list_pre.append([row[0], contact_map[t], n])\n",
    "contact_list_pre = pd.DataFrame(contact_list_pre, columns = [\"PID\", \"Type\", \"Number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_list_mid = []\n",
    "\n",
    "a = clean_surveys[\"uw_mid\"][contact_type]\n",
    "b = clean_surveys[\"uw_mid\"][contact_num]\n",
    "for row_index, row in enumerate(a.iterrows()):\n",
    "    l = row[1]\n",
    "    for col_index,t in enumerate(l):\n",
    "        if (not pd.isna(t)):\n",
    "            n_ori = b.iloc[row_index, col_index]\n",
    "            n = str(n_ori).replace(\" \",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"-\",\"\").replace(\"+\",\"\")\n",
    "            if (n == \"\"): continue\n",
    "            contact_list_mid.append([row[0], contact_map[t], n])\n",
    "contact_list_mid = pd.DataFrame(contact_list_mid, columns = [\"PID\", \"Type\", \"Number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_list_post = []\n",
    "\n",
    "a = clean_surveys[\"uw_post\"][contact_type]\n",
    "b = clean_surveys[\"uw_post\"][contact_num]\n",
    "for row_index, row in enumerate(a.iterrows()):\n",
    "    l = row[1]\n",
    "    for col_index,t in enumerate(l):\n",
    "        if (not pd.isna(t)):\n",
    "            n_ori = b.iloc[row_index, col_index]\n",
    "            n = str(n_ori).replace(\" \",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"-\",\"\").replace(\"+\",\"\")\n",
    "            if (n == \"\"): continue\n",
    "            contact_list_post.append([row[0], contact_map[t], n])\n",
    "contact_list_post = pd.DataFrame(contact_list_post, columns = [\"PID\", \"Type\", \"Number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_list_pre = contact_list_pre.drop_duplicates([\"PID\", \"Type\", \"Number\"])\n",
    "contact_list_mid = contact_list_mid.drop_duplicates([\"PID\", \"Type\", \"Number\"])\n",
    "contact_list_post = contact_list_post.drop_duplicates([\"PID\", \"Type\", \"Number\"])\n",
    "contact_list = pd.concat([contact_list_pre, contact_list_mid, contact_list_post]).drop_duplicates([\"PID\", \"Type\", \"Number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_list.to_csv(surveydir + \"contact_list.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_list = pd.read_csv(surveydir + \"contact_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PID - device id - number - hash value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_call = \"calls.csv\"\n",
    "file_message = \"messages.csv\"\n",
    "file_pid = \"C:/Users/orson/Desktop/Myself/HCI/UWiSchool/Projects/UWEXP/Code/script-input/sensors/pid_device_participants-180630.json\"\n",
    "# file_plugin = \"sefidgar_12_plugin_contacts.xlsx\"\n",
    "file_device = \"aware_device.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_device = pd.read_csv(surveydir + file_device)\n",
    "# df_plugin = pd.read_excel(file_plugin)\n",
    "df_call = pd.read_csv(surveydir + file_call)\n",
    "df_messages = pd.read_csv(surveydir + file_message)\n",
    "with open(file_pid, \"r\") as f:\n",
    "    df_pid = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messages = df_messages.drop_duplicates()\n",
    "df_call = df_call.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_pid_to_deviceid = {}\n",
    "dic_deviceid_to_pid = {}\n",
    "for k, v in df_pid.items():\n",
    "    dic_pid_to_deviceid[int(k)] = v\n",
    "    for vv in v:\n",
    "        dic_deviceid_to_pid[vv] = int(k)\n",
    "for i in contact_list[\"PID\"].tolist():\n",
    "    if (i not in dic_pid_to_deviceid):\n",
    "        dic_pid_to_deviceid[i] = []\n",
    "for i in list(set(df_messages.device_id)):\n",
    "    if (i not in dic_deviceid_to_pid):\n",
    "        dic_deviceid_to_pid[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messages[\"PID\"] = df_messages[\"device_id\"].map(dic_deviceid_to_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_potential_sha1_output(num):\n",
    "    num1 = num[0:3]\n",
    "    num2 = num[3:6]\n",
    "    num3 = num[6:]\n",
    "    ns = []\n",
    "    ns.append(num1 + num2 + num3)\n",
    "    ns.append(num1 + \"-\" + num2 + \"-\" + num3)\n",
    "    ns.append(\"(\" + num1 + \") \" + num2 + \"-\" + num3)\n",
    "    ns.append(num1 + \" \" + num2 + \" \" + num3)\n",
    "    ns.append(\"(\" + num1 + \")\" + num2 + \"-\" + num3)\n",
    "    ns1 = [\"+1 \" + x for x in ns]\n",
    "    ns += ns1\n",
    "    sha1_output = [hashlib.sha1(x.encode()).hexdigest() for x in ns]\n",
    "    return sha1_output\n",
    "# contact_list[\"hash_value\"] = contact_list[\"Number\"].apply(generate_potential_sha1_output)\n",
    "contact_list[\"trace\"] = contact_list[\"Number\"].apply(lambda x : hashlib.sha1(str(x).encode()).hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messagesss = df_messages.merge(contact_list, how = \"left\", on = [\"PID\", \"trace\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messagesss_extacted = df_messagesss[[\"PID\",\"device_id\",\"trace\",\"Type\", \"Number\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messagesss_extacted.to_csv(surveydir + \"messages_merged_type.csv\", index = False,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Scores against qualtrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
