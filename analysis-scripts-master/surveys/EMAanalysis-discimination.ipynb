{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.stats.anova as anova\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Specify the file containing summary discrimination information (i.e. `discriminated` column). Running the `EMAprep` notebook will generate such a file for the relevant surveys (weekly and evening). You can also aggregate them using `EMAaggregation` notebook to get a combined file and scale grouping.\n",
    "\n",
    "Note: weekly reports of discimination ask about the incidents that occured the day before. So affective ratings and stressors of weekly responses reflect the status the day after the discrimination. Substance use questions are asking about the day before so they represent same day status. On the other hand, evening reports of discrimination ask about the incidens of the same day. Therefore, affect ratings and stressors represent the same day status. Subsance use of the morning of the next day are relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this should be rewritten to read from the aggregated data. **TO-DO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "Specify the **absolute path** of the configuration file containing information about: \n",
    "\n",
    " - the file containing summary discrimination information (i.e. `discriminated` column). \\\n",
    " Running the `EMAprep` notebook will generate such a file for the relevant surveys (weekly and evening).\\\n",
    " You can also aggregate them using `EMAaggregation` notebook to get a combined file and scale grouping.\n",
    " - output files where various results (tables and figures) are stored.\n",
    "\n",
    "Note: weekly reports of discimination ask about the incidents that occured the day before.\\\n",
    " So affect ratings and stressors of weekly responses reflect the status the day after the\\\n",
    " discrimination. Substance use questions are asking about the day before so they represent\\\n",
    " same day status. On the other hand, evening reports of discrimination ask about the\\\n",
    " incidens of the same day. Therefore, affect ratings and stressors represent the same day\\\n",
    " status. Subsance use of the morning of the next day are relevant.\n",
    "\n",
    "  \n",
    "\"\"\"\n",
    "#config_file = 'emadiscrimination-config-evening.json'\n",
    "config_file = input(prompt)\n",
    "print('using configurations specified in {}'.format(config_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, 'r') as file_obj:\n",
    "    config = json.load(file_obj)\n",
    "\n",
    "survey = config['survey']\n",
    "data_file = config['data_file']\n",
    "column_grouping_file = config['column_grouping_file']\n",
    "sleep_file = config['additional_files']['sleep']\n",
    "step_file = config['additional_files']['step']\n",
    "discrimination_id_file = config['discrimination_id_file']\n",
    "discrimination_file = config['discrimination_file']\n",
    "discrimination_frq_figure = config['discrimination_frq_figure']\n",
    "discrimination_btw_box = config['discrimination_btw_box']\n",
    "discrimination_dayof_within_box = config['discrimination_dayof_within_box']\n",
    "discrimination_dayafter_within_box = config['discrimination_dayafter_within_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reading data with summary discrimination information from', data_file, 'for survey', survey)\n",
    "additional_str = \"\"\n",
    "for item in config['additional_files']:\n",
    "    additional_str = config['additional_files'][item] + \", \" + additional_str\n",
    "print('the additional files to use for analysis are', additional_str)\n",
    "print('   participant IDs who reported discrimination are stored in', discrimination_id_file)\n",
    "print('   reports of discrimination by PID and date are stored in', discrimination_file)\n",
    "print('   the frequency of different types of discrimination are stored in', discrimination_frq_figure)\n",
    "print('   the box plot of affect ratings between people who reported discrimination and people who did not is stored in', discrimination_btw_box)\n",
    "print('   the box plot of affect ratings of people who reported discrimination on days they reported discrimination vs. days they did not is stored in', discrimination_dayof_within_box)\n",
    "print('   the box plot of affect ratings of people who reported discrimination on days after reports of dicrimination vs. days with no reports of discrimination the prior day is stored in', discrimination_dayafter_within_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read column groupings\n",
    "with open(column_grouping_file, 'r') as file_obj:\n",
    "    scale_grouping = json.load(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read EMA responses\n",
    "responses = pd.read_csv(data_file)\n",
    "#responses.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrimination Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO test of normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO test of homogenity of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-way ANOVA comparing affect ratings between participants who reported discrimination at least once and\n",
    "# those who did not report discrimination at all\n",
    "def between_discrimination(dependent_column_name, resps):\n",
    "    F, p =  stats.f_oneway(resps[resps['discriminated'] == 'YES'][dependent_column_name], \n",
    "                           resps[resps['discriminated'] == 'NO'][dependent_column_name])\n",
    "    #F, p = stats.mannwhitneyu(resps[resps['discriminated'] == 'YES'][dependent_column_name], \n",
    "    #                       resps[resps['discriminated'] == 'NO'][dependent_column_name]) # TO-DO test\n",
    "    print('one-way ANOVA on {}: F = {:.2f}, p = {:.3f}'.format(dependent_column_name, F, p))\n",
    "    print('*****************')\n",
    "    return (F, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeated measure ANOVA comparing affect ratings of participants who reported discrimination\n",
    "# between days they reported discrimination and days they didn't\n",
    "def within_discrimination(dependent_column_name, resps):\n",
    "    model = anova.AnovaRM(resps, \n",
    "                          dependent_column_name, \n",
    "                          'pid', \n",
    "                          within=['discriminated'],\n",
    "                          aggregate_func=np.mean)\n",
    "    res = model.fit()\n",
    "    print('ratings of {}'.format(dependent_column_name))\n",
    "    print(res)\n",
    "    print('*****************')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_box_color(bp, color):\n",
    "    plt.setp(bp['boxes'], color=color)\n",
    "    plt.setp(bp['whiskers'], color=color)\n",
    "    plt.setp(bp['caps'], color=color)\n",
    "    plt.setp(bp['fliers'], color=color, marker = \".\")\n",
    "    plt.setp(bp['medians'], color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO effect size calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discrimination statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfair_subset = responses[responses['discriminated'] == 'YES']\n",
    "no_unfair_subset = responses[responses['discriminated'] == 'NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfair_report_pids = (unfair_subset.sort_values('PID'))['PID'].unique()\n",
    "no_unfair_report_pids = (no_unfair_subset.sort_values('PID'))['PID'].unique()\n",
    "pids_reported_both = np.intersect1d(no_unfair_report_pids, unfair_report_pids)\n",
    "pids_reported_no_unfair = set(no_unfair_report_pids) - set(unfair_report_pids)\n",
    "pids_reported_only_unfair = set(unfair_report_pids) - set(pids_reported_both)\n",
    "print('number of discrimination reports: ', unfair_subset.shape[0])\n",
    "print('people reporting discrimination', unfair_report_pids)\n",
    "print('number of people discriminated against: ', len(unfair_report_pids))\n",
    "print('number of people with no reports of discrimination', len(pids_reported_no_unfair))\n",
    "print('people who reported discrimination in every survey they responded to', pids_reported_only_unfair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} of unfair treatment reports are long'.format(unfair_subset['long'].sum()))\n",
    "print('{} of unfair treatment reports are late'.format(unfair_subset['late'].sum()))\n",
    "print('{} of unfair treatment reports are late and long'.format(unfair_subset[unfair_subset['late']]['long'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfair_subset[unfair_subset['long']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfair_subset[unfair_subset['late']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: take a look at late and long reports of unfair treatment and decide if it fine to include them in further analysis\n",
    "\n",
    "- YSS: UW phase I weekly EMA: all are fine\n",
    "- YSS: UW phase I evening EMA: row 1057 is too long and late; everything else is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_unfair = scale_grouping['discrimination'].copy()\n",
    "columns_unfair.remove('unfair_not')\n",
    "if 'unfair_yesno' in columns_unfair:\n",
    "    columns_unfair.remove('unfair_yesno')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_pid_unfair_report = unfair_subset.groupby('pid').sum()[columns_unfair]\n",
    "per_pid_unfair_report['total'] = per_pid_unfair_report[columns_unfair].sum(axis=1)\n",
    "# TO-DO obtain the exact dates of discrimination for every report and store that alongside this information\n",
    "per_pid_unfair_report.to_csv(discrimination_id_file)\n",
    "per_pid_unfair_report.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICIPANT_NUM=209\n",
    "plotObj = unfair_subset['pid'].plot(kind='hist', bins=PARTICIPANT_NUM)\n",
    "plotObj.set_xlabel('participant ids')\n",
    "print('some people have reported discrimination once but some have reported it multiple times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = per_pid_unfair_report[columns_unfair].sum().sort_values(ascending=False).plot(kind='bar')\n",
    "xlabels = [x.get_text()[7:] for x in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(xlabels)\n",
    "ax.yaxis.grid()\n",
    "ax.set_ylabel('Number of Reports')\n",
    "plt.tight_layout()\n",
    "plt.savefig(discrimination_frq_figure, format = 'pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfair_subset['discrimination_num'] = unfair_subset[columns_unfair].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns = ['pid', 'recorded_date', 'discrimination_num']\n",
    "output_columns.extend(columns_unfair)\n",
    "unfair_subset[output_columns].set_index('pid').to_csv(discrimination_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO create plots similar to those in response characteristics for the following slices\n",
    "# - people who have reported discrimination at least once vs. people who have not repported discrimination\n",
    "# - for people who have reported discrimination on days with reports of discrimination vs. not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using aggregated data\n",
    "#haggregated_file = '/Users/yasaman/UWEXP/analysis-scripts/surveys/results/emaaggregation/aggregated-horizontal-numVal-internalID.csv'\n",
    "haggregated_file = '/Users/yasaman/UWEXP/analysis-scripts/combined/results/bigtable_UWonly.csv'\n",
    "data = pd.read_csv(haggregated_file)\n",
    "unfair_cols = ['unfair_age', 'unfair_appearance', 'unfair_disability', 'unfair_gender', 'unfair_height', \n",
    "               'unfair_income', 'unfair_intelligence', 'unfair_learning', 'unfair_major', 'unfair_national', \n",
    "               'unfair_orientation', 'unfair_religion', 'unfair_weight']\n",
    "discrimination_category_breakdown_fig = '/Users/yasaman/UWEXP/analysis-scripts/surveys/results/discrimination/discrimination_category_breakdown.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_post_pids = [18, 26, 27, 41, 53, 56, 69, 71, 81, 83, 85, 89, 100, 101, 107, 112, 114, 119, 121, 129, 131, 133, 135, \n",
    "                139, 141, 147, 152, 164, 182, 192, 197, 200, 208]\n",
    "data = data[~data['PID'].isin(no_post_pids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data[unfair_cols].sum().sort_values(ascending=False).plot(kind='bar', color=['b']*len(unfair_cols))\n",
    "xlabels = [x.get_text()[7:] for x in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(xlabels, rotation=45, ha='right')\n",
    "ax.yaxis.grid()\n",
    "ax.set_ylabel('Number of Reports')\n",
    "plt.tight_layout()\n",
    "plt.savefig(discrimination_category_breakdown_fig, format = 'png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TO-DO the following sections should be refactored </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discrimination comparisons: affect\n",
    "\n",
    "compare affect ratings for:\n",
    "\n",
    "- individuals who reported discrimination vs. not (btw comparison over avg ratings of each individual in both groups)\n",
    "- days with and without discrimination for individuals who have experienced discrimination (within comparison over avg ratings for individuals who reported discrimination on days they reported discrimination and on days they didn't)\n",
    "- days after reports of discrimination and days no discrimination was reported the prior day (within comparisons over avg ratings for individuals who reported disrimination if discrimination happened the day before vs. not)\n",
    "\n",
    "Between comparisons are indicative of likely chornic correlates of exposure to discrimination while the within comparisons identify short-term correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean affect ratings for \n",
    "# - participants who reported discrimination at least once\n",
    "# - participants who did not report any discrimination\n",
    "between = responses[responses['discriminated'].notnull()]\n",
    "between_affect_avg = between.groupby(['pid'])[scale_grouping['affect']].mean()\n",
    "between_affect_avg.loc[unfair_report_pids, 'discriminated'] = 'YES'\n",
    "between_affect_avg.loc[pids_reported_no_unfair, 'discriminated'] = 'NO'\n",
    "group_means = between_affect_avg.groupby(['discriminated'])[scale_grouping['affect']].mean()\n",
    "print(group_means.T)\n",
    "mean_diff = group_means.loc['YES'] - group_means.loc['NO']\n",
    "print('the difference in affect ratings in people who experienced discrimination vs. those who did not')\n",
    "print('NOTE: positive difference means the rating is larger in the group who experienced discrimination')\n",
    "print('NOTE: ratings range from 1 (not at all) to 5 (extremely) for each item below')\n",
    "print(mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO test assumptions for the applicability of one-way ANOVA\n",
    "result_between = [between_discrimination(scale, between_affect_avg) for scale in scale_grouping[\"affect\"]]\n",
    "\n",
    "# NOTE: discrimination reports are not ranked samplese so I'm not sure if Mann-Whitney U test is applicable here\n",
    "# TO-DO search further and consult with a statistician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = between_affect_avg.boxplot(column=scale_grouping['affect'], \n",
    "                           by='discriminated', \n",
    "                           figsize=(12, 16),\n",
    "                           layout=(int(len(scale_grouping['affect'])/2)+1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminated_yes = [between_affect_avg.loc[unfair_report_pids, scale] for scale in scale_grouping[\"affect\"]]\n",
    "discriminated_no = [between_affect_avg.loc[pids_reported_no_unfair, scale] for scale in scale_grouping[\"affect\"]]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "bpl = plt.boxplot(discriminated_yes, \n",
    "                  positions=np.array(range(len(scale_grouping['affect'])))*2.0-0.4, \n",
    "                  sym='', widths=0.6)\n",
    "bpr = plt.boxplot(discriminated_no, \n",
    "                  positions=np.array(range(len(scale_grouping['affect'])))*2.0+0.4, \n",
    "                  sym='', widths=0.6)\n",
    "set_box_color(bpl, '#D7191C') # colors are from http://colorbrewer2.org/\n",
    "set_box_color(bpr, '#2C7BB6')\n",
    "\n",
    "# draw temporary red and blue lines and use them to create a legend\n",
    "plt.plot([], c='#D7191C', label='Reported >= 1 Discrimination')\n",
    "plt.plot([], c='#2C7BB6', label='Reported No Discrimination')\n",
    "plt.legend(ncol = 2)\n",
    "plt.ylabel('Affect Ratings')\n",
    "plt.xticks(range(0, len(scale_grouping['affect']) * 2, 2), \n",
    "           [scale[5: ] for scale in scale_grouping['affect']])\n",
    "plt.xlim(-1, len(scale_grouping['affect'])*2-1)\n",
    "plt.ylim(0, 5.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(discrimination_btw_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TO-DO same-day analysis is only applicable to daily data. day-after analysis is applicable to both daily and weekly. proper filtering should be in place to ensure only daily data is used for same-day analysis. also, proper calculations should ensure the next-day values are used from daily data for day-after analysis. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean affect ratings of participants who reported discrimination\n",
    "# between days they reported discrimination and days they didn't\n",
    "within = responses[responses['discriminated'].notnull() & responses['pid'].isin(pids_reported_both)]\n",
    "within_unstacked = within.groupby(['pid', 'discriminated'])[scale_grouping['affect']].mean().unstack()\n",
    "group_means = within_unstacked.mean().unstack(level=-1)\n",
    "print(group_means)\n",
    "mean_diff = group_means['YES'] - group_means['NO']\n",
    "print('the difference in affect ratings in the presence and absence of discrimination')\n",
    "print('NOTE: positive difference means the rating is larger when discrimination is reported')\n",
    "print('NOTE: ratings range from 1 (not at all) to 5 (extremely) for each item below')\n",
    "print(mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO test assumptions for the applicability of repeated measure ANOVA\n",
    "result_within = [within_discrimination(scale, within) for scale in scale_grouping['affect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrimination_reported = [within_unstacked[scale]['YES'] for scale in scale_grouping['affect']]\n",
    "discrimination_not_reported = [within_unstacked[scale]['NO'] for scale in scale_grouping['affect']]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "bpl = plt.boxplot(discrimination_reported, \n",
    "                  positions=np.array(range(len(scale_grouping['affect'])))*2.0-0.4, \n",
    "                  sym='', widths=0.6)\n",
    "bpr = plt.boxplot(discrimination_not_reported, \n",
    "                  positions=np.array(range(len(scale_grouping['affect'])))*2.0+0.4, \n",
    "                  sym='', widths=0.6)\n",
    "set_box_color(bpl, '#D7191C') # colors are from http://colorbrewer2.org/\n",
    "set_box_color(bpr, '#2C7BB6')\n",
    "\n",
    "# draw temporary red and blue lines and use them to create a legend\n",
    "plt.plot([], c='#D7191C', label='Discrimination Reported')\n",
    "plt.plot([], c='#2C7BB6', label='Discrimination NOT Reported')\n",
    "plt.legend(ncol = 2)\n",
    "plt.ylabel('Affect Ratings')\n",
    "plt.xticks(range(0, len(scale_grouping['affect']) * 2, 2), \n",
    "           [scale[5: ] for scale in scale_grouping['affect']])\n",
    "plt.xlim(-1, len(scale_grouping['affect'])*2-1)\n",
    "plt.ylim(0, 5.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(discrimination_dayof_within_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discrimination comparisons: alcohol consumption\n",
    "\n",
    "compare alcohol consumption for:\n",
    "\n",
    "- individuals who reported discrimination vs. not (btw comparisons of number of reports of alcohol consumption by individuals in each group)\n",
    "- after reports of discrimination vs. not (within comparison of the number of reports of alcohol consumptions for individuals who reported discrimination on days they reported discrimination and days they didn't)\n",
    "\n",
    "Between comparisons are indicative of likely chornic correlates of exposure to discrimination while the within comparisons identify short-term correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# between comparisons\n",
    "columns_substance = ['stimulant_yesno', 'alcohol_yesno', 'any_drug', 'any_substance']\n",
    "between = responses[(((responses['survey'] == 'weekly') | (responses['survey'] == 'morning'))\n",
    "                      & (responses['discriminated'].notnull()))]\n",
    "between['stimulant_yesno'] = between['stimulant_yesno'].map({2:0, 1:1})\n",
    "between['alcohol_yesno'] = between['alcohol_yesno'].map({2:0, 1:1})\n",
    "between_substance_frq = between.groupby(['pid'])[columns_substance].sum()\n",
    "between_substance_frq.loc[unfair_report_pids, 'discriminated'] = 'YES'\n",
    "between_substance_frq.loc[pids_reported_no_unfair, 'discriminated'] = 'NO'\n",
    "group_means = between_substance_frq.groupby(['discriminated'])[columns_substance].mean()\n",
    "group_means\n",
    "mean_diff = group_means.loc['YES'] - group_means.loc['NO']\n",
    "print('the difference in average frequency of substance use in people who experienced discrimination vs. those who did not')\n",
    "print('NOTE: positive difference means the average frequency is larger in the group who experienced discrimination')\n",
    "print(mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO test assumptions for the applicability of one-way ANOVA\n",
    "result_between = [between_discrimination(item, between_substance_frq) for item in columns_substance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-Do plots for alcohol consumption between groups d\n",
    "# TO-DO alcohol consumption (within comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discrimination comparisons: reports of stressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discrimination comparisons: sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep = pd.read_csv(sleep_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep.loc[sleep['PID'].isin(unfair_report_pids), 'discriminated'] = 'YES'\n",
    "sleep.loc[sleep['PID'].isin(pids_reported_no_unfair), 'discriminated'] = 'NO'\n",
    "print('people with reports of discrimination whose sleep data is unavailable: {}'.format(set(unfair_report_pids) - set(sleep['PID'].unique())))\n",
    "print('people with no reports of discrimination whose sleep data is unavailable: {}'.format(pids_reported_no_unfair - set(sleep['PID'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_columns = ['totalTimeInBed', 'totalMinutesAsleep', 'minutesAwake', 'minutesAsleep', 'efficiency']\n",
    "between_sleep_avg = sleep[sleep['isMainSleep'] == True].groupby(['PID'])[sleep_columns].mean()\n",
    "between_sleep_avg.loc[set(unfair_report_pids) & set(sleep['PID'].unique()), 'discriminated'] = 'YES'\n",
    "between_sleep_avg.loc[pids_reported_no_unfair & set(sleep['PID'].unique()), 'discriminated'] = 'NO'\n",
    "group_means = between_sleep_avg.groupby(['discriminated'])[sleep_columns].mean()\n",
    "print(group_means.T)\n",
    "mean_diff = group_means.loc['YES'] - group_means.loc['NO']\n",
    "print('the difference in sleep metrics in people who experienced discrimination vs. those who did not')\n",
    "print('NOTE: positive difference means metris are larger in the group who experienced discrimination')\n",
    "print('all time measures are in minutes')\n",
    "print(mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_between = [between_discrimination(metric, between_sleep_avg) for metric in sleep_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discrimination comparisons: step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = pd.read_csv(step_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_columns = ['steps']\n",
    "between_step_avg = step.groupby(['PID'])[step_columns].mean()\n",
    "between_step_avg.loc[set(unfair_report_pids) & set(step['PID'].unique()), 'discriminated'] = 'YES'\n",
    "between_step_avg.loc[pids_reported_no_unfair & set(step['PID'].unique()), 'discriminated'] = 'NO'\n",
    "group_means = between_step_avg.groupby(['discriminated'])[step_columns].mean()\n",
    "print(group_means.T)\n",
    "mean_diff = group_means.loc['YES'] - group_means.loc['NO']\n",
    "print('the difference in step metrics in people who experienced discrimination vs. those who did not')\n",
    "print('NOTE: positive difference means metris are larger in the group who experienced discrimination')\n",
    "print('all time measures are in minutes')\n",
    "print(mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_between = [between_discrimination(metric, between_step_avg) for metric in step_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
