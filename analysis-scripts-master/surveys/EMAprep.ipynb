{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Specify the configuration file name that contains information about:\n",
    "\n",
    "- the type of the EMA survey (weekly, morning, midday1, midday2, evening)\n",
    "- amount by which the recorded time of survey responses should be shifted to become PT (UW phase I data is in MT zone).\n",
    "- the cutoff for a response being considered empty\n",
    "- the input files where column and scale names are specified.\n",
    "- output files where various results (tables and figures) are stored.\n",
    "\n",
    "Note: the input files can be found in script-input repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "Specify the **absolute path** of the configuration file containing information about:\n",
    "\n",
    "- the type of the EMA survey (weekly, morning, midday1, midday2, evening)\n",
    "- amount by which the recorded time of survey responses should be shifted to become PT (UW phase I data is in MT zone).\n",
    "- the cutoff for a response being considered empty\n",
    "- the input files where column and scale names are specified.\n",
    "- output files where various results (tables and figures) are stored.\n",
    "\n",
    "Note: the input files can be found in script-input repository.\n",
    "\n",
    "Example (find a sample in script-input repository): emaprep-config-evening.json \n",
    "\n",
    "Tips:\n",
    "\n",
    "- Place your configuration files in the same directory as this notebook.\n",
    "- Use a different configuration file for each different analysis rather than modifying a single configuration file.\n",
    "  For example, have separate files for weekly surveys and EMA surveys.\n",
    "\n",
    "  \n",
    "\"\"\"\n",
    "#config_file = 'emaprep-config-evening.json'\n",
    "config_file = input(prompt)\n",
    "print('using configurations specified in {}'.format(config_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, 'r') as file_obj:\n",
    "    config = json.load(file_obj)\n",
    "\n",
    "institution = config['institution']\n",
    "survey = config['survey']\n",
    "data_file = config['data_file']\n",
    "timeshift = config['timeshift']\n",
    "survey_date_file = config['survey_date_file']\n",
    "empty_count_threshold = config['empty_count_threshold']\n",
    "column_name_file = config['column_name_file']\n",
    "column_grouping_file = config['column_grouping_file']\n",
    "cleaned_data_file = config['cleaned_data_file']\n",
    "# TO-DO there should not be a separate file for discrimination, substance use, or stressor info. \n",
    "#       There should be one files at the end that captures these all.\n",
    "data_with_discrimination_info = config['data_with_discrimination_info']\n",
    "data_with_substance_info = config['data_with_substance_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('processing survey', survey, 'of', institution, '...')\n",
    "print('   data is obtained from', data_file)\n",
    "print('   the times recorded in survey responses are shifted by', timeshift, 'hours to reflect PT/PST time.')\n",
    "print('   the response dates are matched against the survey schedule dates recorded in ', survey_date_file)\n",
    "print('   any response with more than', empty_count_threshold, 'empty items is removed from further processing.')\n",
    "print('  ', column_name_file, 'provides the more readable column names; it is a mapping between column names in the data file and the more readable names.')\n",
    "print('  ', column_grouping_file,'provides the grouping of columns for further score calculations.')\n",
    "print('   data is stored in', cleaned_data_file, 'after clean-up.')\n",
    "print('   data with discrimination summary column is stored in', data_with_discrimination_info)\n",
    "print('   data with substance use summary columns is stored in', data_with_substance_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read column names\n",
    "with open(column_name_file, 'r') as file_obj:\n",
    "    columns = file_obj.readlines()\n",
    "columns = [column.strip() for column in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read column groupings\n",
    "with open(column_grouping_file, 'r') as file_obj:\n",
    "    scale_grouping = json.load(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read EMA responses\n",
    "responses = pd.read_csv(data_file, header=None, names=columns, skiprows=2)\n",
    "responses['institution'] = institution\n",
    "responses['survey'] = survey\n",
    "#responses.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up - remove invalid responses\n",
    "# according to qualtrics (https://goo.gl/p4g16k):\n",
    "#    status ==  0 --> normal\n",
    "#    status ==  1 --> preview\n",
    "#    status ==  2 --> test (NA in our data)\n",
    "#    status ==  4 --> imported (NA in our data)\n",
    "#    status ==  8 --> spam (e.g. because of duplicate submissions)\n",
    "#    status == 16 --> offline (NA in our data)\n",
    "valid = responses['status'] == 0\n",
    "print('removing {} response(s) with invalid status'.format(responses.shape[0] - sum(valid)))\n",
    "responses = responses[valid]\n",
    "#responses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up - remove unfinished responses\n",
    "# according to qualtrics (https://goo.gl/p4g16k):\n",
    "#    finished == 0 --> closed without completion (progress < 100)\n",
    "#    finished == 1 --> submitted (progress == 100)\n",
    "finished = responses['finished'] == 1\n",
    "print('removing {} unfinished response(s)'.format(responses.shape[0] - sum(finished)))\n",
    "responses = responses[finished]\n",
    "#responses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_survey = []\n",
    "for item in scale_grouping:\n",
    "    if(item == 'time' or item == 'default' or item == 'identifier'):\n",
    "        continue\n",
    "    columns_survey.extend(scale_grouping[item])\n",
    "#print(columns_survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up - remove empty resposnes\n",
    "empty = responses[columns_survey].isnull().all(axis=1)\n",
    "print('removing {} empty response(s)'.format(sum(empty)))\n",
    "responses = responses[~empty]\n",
    "print('remaining empty response(s): {}'.format(sum(responses[columns_survey].isnull().all(axis=1))))\n",
    "#responses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up - remove near empty responses\n",
    "responses['empty_count'] = responses[columns_survey].isnull().sum(axis=1)\n",
    "print('removing {} near empty response(s)'.format(sum(responses['empty_count'] >= empty_count_threshold)))\n",
    "responses = responses[responses['empty_count'] < empty_count_threshold]\n",
    "#responses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up - tag long submissions\n",
    "# long submissions are submissions with duration longer than two standard error of the mean of all the submissions\n",
    "# TO-DO double check with Jen if we want to have a fixed cut-off for each survey type or it is fine to have this\n",
    "#       data-driven cut-off regime\n",
    "min_duration = responses['duration'].min()\n",
    "max_duration = responses['duration'].max()\n",
    "mean_duration = responses['duration'].mean()\n",
    "std_duration = responses['duration'].std()\n",
    "stderr_duration = std_duration / math.sqrt(responses.shape[0])\n",
    "explanation = 'time on survey varies'\n",
    "print('{} from {} (sec) to {} (sec) (M = {:.3f}, std = {:.3f}, std_err = {:.3f})'.format(explanation,\n",
    "                                                                                         min_duration,\n",
    "                                                                                         max_duration,\n",
    "                                                                                         mean_duration,\n",
    "                                                                                         std_duration,\n",
    "                                                                                         stderr_duration))\n",
    "cut_off = mean_duration + 2 * std_duration\n",
    "responses['long'] = responses['duration'] >= cut_off\n",
    "print('tagged {} response(s) longer than two standard deviation of the mean (>= {:.3f} secs)'.format(responses[responses['long']].shape[0],\n",
    "                                                                                                     cut_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('distribution of responses NOT considered long')\n",
    "ax = responses[~responses['long']].hist(column='duration', \n",
    "                                        bins=100, \n",
    "                                        grid=False, \n",
    "                                        figsize=(12,8), \n",
    "                                        color='#86bf91', \n",
    "                                        zorder=2, \n",
    "                                        rwidth=0.9)\n",
    "\n",
    "ax = ax[0]\n",
    "for x in ax:\n",
    "\n",
    "    # Despine\n",
    "    x.spines['right'].set_visible(False)\n",
    "    x.spines['top'].set_visible(False)\n",
    "    x.spines['left'].set_visible(False)\n",
    "\n",
    "    # Switch off ticks\n",
    "    x.tick_params(axis=\"both\", \n",
    "                  which=\"both\", \n",
    "                  bottom=\"off\", \n",
    "                  top=\"off\", \n",
    "                  labelbottom=\"on\", \n",
    "                  left=\"off\", \n",
    "                  right=\"off\", \n",
    "                  labelleft=\"on\")\n",
    "\n",
    "    # Draw horizontal axis lines\n",
    "    vals = x.get_yticks()\n",
    "    for tick in vals:\n",
    "        x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "\n",
    "    # Remove title\n",
    "    x.set_title(\"\")\n",
    "\n",
    "    # Set x-axis label\n",
    "    x.set_xlabel(\"Response Duration Not Considered Long (Seconds)\", labelpad=20, weight='bold', size=12)\n",
    "\n",
    "    # Set y-axis label\n",
    "    x.set_ylabel(\"Responses\", labelpad=20, weight='bold', size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion - convert the date columns to datetime objects and adjust for time zone different\n",
    "columns_date = scale_grouping['time'].copy()\n",
    "columns_date.remove('duration')\n",
    "responses[columns_date] = responses[columns_date].apply(pd.to_datetime)\n",
    "responses[columns_date] = responses[columns_date] + pd.DateOffset(hours=timeshift) # records of phase I are in MT\n",
    "responses[columns_date] = responses[columns_date].apply(pd.to_datetime) # to ensure columns date are datetime objects\n",
    "# NOTE: DateOffset data is stored as object data rahter than datetime data\n",
    "#       I compared the results of time related calculations below with and without the additional type conversion.\n",
    "#       Eveything remained the same in terms of late tagging and values of not_late_min, not_late_max, not_late_mean,\n",
    "#       not_late_std, and not_late_stderr.\n",
    "#responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multisubmission removal - this is relevant if there are multiple submissions by the same person on the same date\n",
    "# Unfortunately, Qualtrics status code has failed to identify some of the multi submission (e.g. because there \n",
    "# were made from different IP addresses).\n",
    "responses['date_'] = responses['start_date'].dt.date\n",
    "responses['date_'] = responses['date_'].astype('datetime64[ns]')\n",
    "# NOTE I decided not to correct for start_time before 10am. While an outlier in terms of submission \n",
    "#      time the responses are arguably valid. Also, it is unclear by how much I should correct and\n",
    "#      with something as long as 3 hours I may mess up with the logic below for removing the multi\n",
    "#      submissions (e.g. if the correction causes the later response to go before the earlier one)\n",
    "cols_ = list(responses.columns)\n",
    "cols_.remove('date_')\n",
    "cols_.remove('PID')\n",
    "cols_.remove('empty_count')\n",
    "# NOTE of multi responses pick the one w/ smaller number of empty questions, earlier start_date, and shorter duration\n",
    "responses = responses.groupby(by=['date_', 'PID']).apply(lambda x: x.sort_values(\n",
    "                                                                     by=['empty_count', 'start_date', 'duration'],\n",
    "                                                                     ascending=[True, True, True]).iloc[0])[cols_].reset_index()\n",
    "# NOTE this is a special case of a very late submission. Given I did not want to correct late submissions here \n",
    "# and this was the only case where this is happening\n",
    "if(survey == 'weekly'):\n",
    "    ind = responses[(responses['PID'] == 83) & (responses['date_'] == datetime.date(2018, 1, 22))].index\n",
    "    responses = responses.drop(ind)\n",
    "\n",
    "counter = responses.groupby(['date_', 'PID']).size()\n",
    "multisub = counter[counter > 1] \n",
    "print('number of multi submissions after cleanup: {} (must be zero)'.format(len(multisub)))\n",
    "responses = responses.drop(columns=['date_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the indices of \n",
    "def check_late(srvy, resps):\n",
    "    \"\"\"\\\n",
    "    returns the boolean indices of respones in dataframe resps that are considered as acceptable (i.e. not late)\n",
    "    for survey srvy\n",
    "    \"\"\"\n",
    "    # NOTE: it is important to use end_date rather that recorded_date. While the former indicates the time that \n",
    "    #       a responder submits her/his response, the latter is the time when the data appears on Qualtrics surveys. \n",
    "    #       When the connection is slow, recorded_date can be much later than end_date\n",
    "    ind  = (resps['start_date'] >= srvy['from']) & (resps['end_date'] <= srvy['to'])\n",
    "    #print(srvy['from'], ' - ', srvy['to'], ' : ', resps[ind]['late'].size)\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up - tag late submissions\n",
    "# acceptable submissions are submissions that started on the survey date and after start time and have been recorded\n",
    "# on the survey date and before the expiry date. Any other submission is late.\n",
    "survey_dates = pd.read_csv(survey_date_file)\n",
    "survey_dates['from'] = pd.to_datetime(survey_dates['date'] + ' ' + survey_dates['start'])\n",
    "survey_dates['to'] = pd.to_datetime(survey_dates['date'] + ' ' + survey_dates['expiry'])\n",
    "survey_dates['date'] = pd.to_datetime(survey_dates['date'])\n",
    "survey_dates['start'] = pd.to_datetime(survey_dates['start'],format= '%H:%M:%S').dt.time\n",
    "survey_dates['expiry'] = pd.to_datetime(survey_dates['expiry'],format= '%H:%M:%S').dt.time\n",
    "inds = survey_dates[survey_dates['type'] == survey].apply(lambda x : check_late(x, responses), axis = 1)\n",
    "responses['late'] = True\n",
    "responses.loc[inds.T.any(axis=1), 'late'] = False\n",
    "print('tagged {} late response(s)'.format(responses[responses['late']].shape[0]))\n",
    "# TO-DO consider a 10 minute of grace period\n",
    "# TO-DO test: systematically evaluate entries with late = True and late = False if they make sense\n",
    "#       (I eye balled the results for a few dattes and they were OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_late_min = responses[~responses['late']]['duration'].min()\n",
    "not_late_max = responses[~responses['late']]['duration'].max()\n",
    "not_late_mean = responses[~responses['late']]['duration'].mean()\n",
    "not_late_std = responses[~responses['late']]['duration'].std()\n",
    "not_late_stderr = not_late_std / math.sqrt(responses[~responses['late']].shape[0])\n",
    "explanation = 'time on survey for responses that are NOT late varies'\n",
    "print('{} from {} (sec) to {} (sec) (M = {:.3f}, std = {:.3f}, std_err = {:.3f})'.format(explanation,\n",
    "                                                                                         not_late_min,\n",
    "                                                                                         not_late_max,\n",
    "                                                                                         not_late_mean,\n",
    "                                                                                         not_late_std,\n",
    "                                                                                         not_late_stderr))\n",
    "print('{} late response(s) are also long'.format(responses[responses['late']]['long'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping for tagging information\n",
    "scale_grouping['tags'] = ['survey', 'long', 'late']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up - remove all the columns no longer needed\n",
    "columns_drop = scale_grouping['default']\n",
    "responses.drop(columns=columns_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the cleaned up data\n",
    "responses.to_csv(cleaned_data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO test if values for each Likert Style or Yes/No scale fall in the expected range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO test if values add up within each scale\n",
    "#       e.g. did not experience stress and any other stressor are not both selected)\n",
    "#       or did not use drug and a type of drug are not selected at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO check that unfair_not and other columns within discrimination are not checked at the same time \n",
    "# TO-DO similarly check that demand_not and other columns within demand are not checked at the same time\n",
    "# TO-DO similarly check that alcohol_yesno as no and alcohol_amount or alcohol_duration don't have values at the same time\n",
    "# TO-DO similarly check that drug_not and other columns within drug are not checked at the same time\n",
    "# TO-DO similarly check that alcohol_yesno as no or drug_not do not co-occure with substance_negative or substance_alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compliance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO prepare the study size per survey date\n",
    "#       - can add this to the survey date file and rename it as survey_file\n",
    "#       - or can create a separate file containing this information\n",
    "# TO-DO find the number of unique valid responses (not long, not late) for each survey\n",
    "#       - create a function similar to check_late that returns the number of unique and not late responses\n",
    "#       - similarly use apply and lambda to get the number \n",
    "# TO-DO the compliance rate for each date is the ratio of unique number of responses over the study size\n",
    "# TO-DO run t-test to compare weekly rates (Sun vs. Wed) and ANOVA to compare daily rates (morning, midday1, midday2, evening)\n",
    "# TO-DO find whose responses are missing on each survey date\n",
    "#       - prepare a table where active PID's are listed for each survey date\n",
    "#       - join these PID's with the unique PID's from responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_data(completions):\n",
    "    completions = completions[completions['miss count'].notnull()]\n",
    "    number_of_responses_expected = completions.shape[0] * 4\n",
    "    number_of_response_missing = completions['miss count'].sum()\n",
    "    rate = 100 - number_of_response_missing / number_of_responses_expected * 100\n",
    "    return pd.Series({'missing': number_of_response_missing, 'expected': number_of_responses_expected, 'rate': rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary analysis of completion rates based on compliance status obtained through excel for daily surveys\n",
    "# if completion is irrelevant to a participant on a certain date (e.g. because they have dropped out of the \n",
    "# study), there is NA in the excel files. This is read as null in the pandas dataframe using read_csv command\n",
    "# below. \n",
    "daily_completions = pd.read_csv('/Users/yasaman/Downloads/data/ema-daily-completions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID_drop = [18, 26, 27, 41, 53, 56, 69, 71, 81, 83, 85, 89, 100, 101, 107, 112, 114, 119, 121, 129, 131, \n",
    "            133, 135, 139, 141, 147, 152, 164, 182, 192, 197, 200, 208]\n",
    "daily_completions = daily_completions[~daily_completions['PID'].isin(PID_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_completions = daily_completions.groupby(by=['date']).apply(completion_data).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_completions.to_csv('/Users/yasaman/Downloads/results/ema-daily-completion-rates_176.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary analysis of weekly average of completions rates\n",
    "rates = pd.read_csv('/Users/yasaman/Downloads/results/completion_rates_176.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rates = rates.groupby('week').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rates.to_csv('/Users/yasaman/Downloads/results/average_completion_rates_176.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Characteristics\n",
    "\n",
    "distribution of responses for items of the following scales\n",
    "\n",
    "- affect (anxiety, depression, frustration, happiness, being overwhelmed, lonliness, social interactions)\n",
    "- stress\n",
    "- coping\n",
    "- substance use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TO-DO for affect scales, create the bar plot over the response range (1 (not at all) to 5 (extremely))\n",
    "# TO-DO for stress, create a bar plot for different kinds of stressors\n",
    "#       also create a bar plot over the response range for the stress forecast\n",
    "# TO-DO for coping, create a bar plot for different coping skills\n",
    "#       also create a bar plot over the response range for effectiveness\n",
    "# TO-DO consider other evening survey metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TO-DO remove this once it is implemented in EMAaggregation for both horizontal and vertical aggregation. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO only applicable to weekly and evening surveys; make sure to condition the following steps on survey type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare discrimination data\n",
    "columns_unfair = scale_grouping['discrimination'].copy()\n",
    "columns_unfair.remove('unfair_not')\n",
    "if 'unfair_yesno' in columns_unfair:\n",
    "    # NOTE: responses to weekly EMA's of phase I are only valid for discrimination analysis if unfair_yesno column is\n",
    "    #       NULL while at least another unfair column is not NULL\n",
    "    columns_unfair.remove('unfair_yesno')\n",
    "    unfair_reported = (responses['unfair_yesno'].isnull() \n",
    "                       & responses[columns_unfair].notnull().any(axis=1))\n",
    "    no_unfair_reported = (responses['unfair_yesno'].isnull()\n",
    "                          & responses[columns_unfair].isnull().all(axis=1)) # TO-DO test\n",
    "    #print('valid dates for discrimination analysis\\n', \n",
    "    #       responses[unfair_reported]['recorded_date'].dt.date.unique())\n",
    "else:\n",
    "    unfair_reported = responses[columns_unfair].notnull().any(axis=1)\n",
    "    no_unfair_reported = responses[columns_unfair].isnull().all(axis=1) # TO-DO test\n",
    "responses.loc[unfair_reported, 'discriminated'] = 'YES'\n",
    "responses.loc[no_unfair_reported, 'discriminated'] = 'NO'\n",
    "unfair_subset = responses[unfair_reported]\n",
    "#unfair_subset = unfair_subset.sort_values('PID')\n",
    "no_unfair_subset = responses[no_unfair_reported]\n",
    "#no_unfair_subset = no_unfair_subset.sort_values('PID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store discrimination information together with the data if applicable\n",
    "responses.to_csv(data_with_discrimination_info, index=False) # TO-DO this should be done at the end after all aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alcohol consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare substance use data\n",
    "if survey == 'weekly' or survey == 'morning':\n",
    "    responses['any_drug'] = responses[scale_grouping['substance'][6:16]].any(axis=1) | (responses['drug_yesno'] == 1)\n",
    "    responses['any_substance'] = (responses['any_drug'] \n",
    "                                  | (responses['stimulant_yesno'] == 1)\n",
    "                                  | (responses['alcohol_yesno'] == 1))\n",
    "    responses.to_csv(data_with_substance_info, index=False) # TO-DO this should be done at the end after all aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
