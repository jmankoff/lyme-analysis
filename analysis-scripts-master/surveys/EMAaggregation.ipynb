{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Specify the configuration file name that contains information about:\n",
    "\n",
    "- surveys being aggregated (their name, data file path, and column grouping file path)\n",
    "- study dates to project aggregated information on\n",
    "- horizontal and vertical aggregation column list and grouping\n",
    "- file path for storing horizonally and vertically aggregated information for the study period\n",
    "\n",
    "Note: horizontal aggregation produces a fixed table in terms of columns and rows. It is filled based on the avaialability of data from surveys being aggregated. For examples, it has affect colunmns for all surveys (weekly, morning, midday1, midday2, and evening). If only weekly and evening surveys are included in the list of surveys being aggregated, only their relevant columns are being aggregated.\n",
    "\n",
    "Note: vertical aggregated produces a fixed table in terms of columns. However, rows vary depending on the surveys being aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "Specify the **absolute path** of the configuration file containing information about:\n",
    "\n",
    "- surveys being aggregated (their name, cleaned data file path, and column grouping file path)\n",
    "- study dates to project aggregated information on\n",
    "- horizontal and vertical aggregation column list and grouping\n",
    "- file path for storing horizonally and vertically aggregated information for the study period\n",
    "\n",
    "Note: horizontal aggregation produces a fixed table in terms of columns and rows. It is filled \\\n",
    "based on the avaialability of data from surveys being aggregated. For examples, it has affect \\\n",
    "colunmns for all surveys (weekly, morning, midday1, midday2, and evening). If only weekly and \\\n",
    "evening surveys are included in the list of surveys being aggregated, only their relevant columns\\\n",
    "are being aggregated.\n",
    "\n",
    "Note: vertical aggregated produces a fixed table in terms of columns. However, rows vary depending \\\n",
    "on the surveys being aggregated.\n",
    "\n",
    "Example (find a sample in script-input repository): emaaggregation-config.json\n",
    "\n",
    "Tips:\n",
    "\n",
    "- Place your configuration files in the same directory as this notebook.\n",
    "- Use a different configuration file for each different analysis rather than modifying a single configuration file.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#config_file = 'emaaggregation-config.json'\n",
    "config_file = input(prompt)\n",
    "print('using configurations specified in {}'.format(config_file))\n",
    "#/Users/yasaman/UWEXP/analysis-scripts/surveys/emaaggregation-config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, 'r') as file_obj:\n",
    "    config = json.load(file_obj)\n",
    "\n",
    "institution = config['institution']\n",
    "surveys = config['surveys']\n",
    "id_range = config['id_range']\n",
    "study_dates = config['study_dates']\n",
    "h_aggregation_file = config['h_aggregation_file']\n",
    "h_aggregation_column_grouping_file = config['h_aggregation_column_grouping_file']\n",
    "v_aggregation_file = config['v_aggregation_file']\n",
    "v_aggregation_column_grouping_file = config['v_aggregation_column_grouping_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_names = ''\n",
    "survey_files = ''\n",
    "survey_grouping_files = ''\n",
    "for survey in surveys:\n",
    "    survey_names = survey_names + survey['name'] + ', '\n",
    "    survey_files = survey_files + '   ' + survey['data_file'] + '\\n'\n",
    "    survey_grouping_files = survey_grouping_files + '   ' + survey['column_grouping_file'] + '\\n'\n",
    "print('aggregating', survey_names, 'surveys of', institution, '...')\n",
    "print('data is obtained from the following files respectively:')\n",
    "print(survey_files, end='')\n",
    "print('grouping of data columns is obtained from the following files respectively:')\n",
    "print(survey_grouping_files, end='')\n",
    "print('data is from ids in range', id_range[0], 'to', id_range[1], '.')\n",
    "print('aggregation happens between {}-{:02d}-{:02d} and {}-{:02d}-{:02d}'.format(study_dates['start']['year'],\n",
    "                                                                                 study_dates['start']['month'],\n",
    "                                                                                 study_dates['start']['day'],\n",
    "                                                                                 study_dates['end']['year'],\n",
    "                                                                                 study_dates['end']['month'],\n",
    "                                                                                 study_dates['end']['day']))\n",
    "print('horizontal aggregation is stored in', h_aggregation_file)\n",
    "print('the columns for horizontally aggregated data can be obtained/should match that listed in various groupings of file', h_aggregation_column_grouping_file)\n",
    "print('vertical aggregation is stored in', v_aggregation_file)\n",
    "print('the columns for vertically aggregated data can be obtained/should match that listed in various groupings of file', v_aggregation_column_grouping_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_affect(name, data, columns, dates_ids):\n",
    "    cols = ['date', 'PID']\n",
    "    cols.extend(columns)\n",
    "    data = pd.merge(dates_ids, data[cols], on=['date', 'PID'], how='left')\n",
    "    # NOTE I had to separate out merging and resetting index and reset index after renming columns because\n",
    "    #      with data = pd.merge(dates_ids, data[cols], on=['date', 'PID'], how='left').set_index(['date', 'PID'])\n",
    "    #      date ended up becoming datetime rather than remaining date. This messed up with the concatenation as\n",
    "    #      other scales have date type and indexes don't match any more.\n",
    "    renaming = {col:col+'_'+name for col in columns}\n",
    "    data.rename(index=str, columns=renaming, inplace=True)\n",
    "    data = data.set_index(['date', 'PID'])\n",
    "    #print('{} - affect - size of perpped data: {}'.format(name, data.shape[0]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_discrimination(name, data, columns, dates_ids):\n",
    "    cols = ['start_date', 'PID']\n",
    "    cols.extend(columns)\n",
    "    data = data[cols]\n",
    "    \n",
    "    # shift the start date of surveys that are before 10am to the day before. these are late submissions\n",
    "    # NOTE no survey can be started between midnight and 10am if the participant have not messed up \n",
    "    #      with the link. surveys of UW phase I where all sent after 10am and where at most valid to \n",
    "    #      answer before the midnight of the same day\n",
    "    data.loc[data['start_date'].dt.time < datetime.time(10, 0, 0), 'start_date'] = \\\n",
    "        data.loc[data['start_date'].dt.time < datetime.time(10, 0, 0), 'start_date'] - pd.Timedelta('12 hour') \n",
    "    \n",
    "    # call add_date based on the adjusted start_time\n",
    "    add_date(data, 'start_date')\n",
    "    \n",
    "    # adjust the dates for weekly surveys to the day before\n",
    "    if(name == 'weekly'):\n",
    "        data.loc[:, 'date'] = data['date'] - pd.Timedelta('1 day')\n",
    "    \n",
    "    cols_unfair = columns.copy()\n",
    "    cols_unfair.remove('unfair_not')\n",
    "    if 'unfair_yesno' in cols_unfair: # weekly survey of UW phase I originally had this column\n",
    "        # NOTE: responses to weekly EMA's of UW phase I are only valid for discrimination analysis \n",
    "        #      if unfair_yesno column is NULL while at least another unfair column is not NULL.\n",
    "        cols_unfair.remove('unfair_yesno')\n",
    "        unfair_reported = (data['unfair_yesno'].isnull() & data[cols_unfair].notnull().any(axis=1))\n",
    "        no_unfair_reported = (data['unfair_yesno'].isnull() & data[cols_unfair].isnull().all(axis=1))\n",
    "    else:\n",
    "        unfair_reported = data[cols_unfair].notnull().any(axis=1)\n",
    "        no_unfair_reported = data[cols_unfair].isnull().all(axis=1)\n",
    "    \n",
    "    data.loc[unfair_reported, 'discriminated'] = 'YES'\n",
    "    data.loc[no_unfair_reported, 'discriminated'] = 'NO'\n",
    "    \n",
    "    cols = ['date', 'PID', 'discriminated']\n",
    "    cols.extend(columns)\n",
    "    \n",
    "    data = pd.merge(dates_ids, data[cols], on=['date', 'PID'], how='left').set_index(['date', 'PID'])\n",
    "    #print('{} - discrimination - size of perpped data: {}'.format(name, data.shape[0]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_discrimination_cmu(name, data, columns, dates_ids):\n",
    "    cols = ['start_date', 'PID']\n",
    "    cols.extend(columns)\n",
    "    data = data[cols]\n",
    "    \n",
    "    # shift the start date of surveys that are before 10am to the day before. these are late submissions\n",
    "    # NOTE no survey can be started between midnight and 10am if the participant have not messed up \n",
    "    #      with the link. surveys of UW phase I where all sent after 10am and where at most valid to \n",
    "    #      answer before the midnight of the same day\n",
    "    data.loc[data['start_date'].dt.time < datetime.time(9, 0, 0), 'start_date'] = \\\n",
    "        data.loc[data['start_date'].dt.time < datetime.time(9, 0, 0), 'start_date'] - pd.Timedelta('12 hour') \n",
    "    # NOTE: I have no idea about the timing of surveys for CMU phase II and I have not cleaned any data for \n",
    "    #       being late according to such timing. I'm going to assume the values I see at midnight for evening\n",
    "    #       surveys belong to the day before. I use 8 instead of 10 as it seems CMU surveys were sent out at 9am.\n",
    "    #       9am is the time that I see in the data.\n",
    "    \n",
    "    # call add_date based on the adjusted start_time\n",
    "    add_date(data, 'start_date')\n",
    "    \n",
    "    #per_date = data.groupby(by=['PID', 'date']).size()\n",
    "    #per_date = per_date[per_date > 1]\n",
    "    #print(per_date) # to fine the late items that after adjustment become problematic as multiple submissions\n",
    "    \n",
    "    # adjust the dates for weekly surveys to the day before\n",
    "    if(name == 'weekly'):\n",
    "        data.loc[:, 'date'] = data['date'] - pd.Timedelta('1 day')\n",
    "    \n",
    "    unfair_reported = (data['unfair_type'] < 14) & (data['unfair_type'] > 0)\n",
    "    no_unfair_reported = data['unfair_type'] == 14\n",
    "    \n",
    "    data.loc[unfair_reported, 'discriminated'] = 'YES'\n",
    "    data.loc[no_unfair_reported, 'discriminated'] = 'NO'\n",
    "    \n",
    "    cols = ['date', 'PID', 'discriminated']\n",
    "    cols.extend(columns)\n",
    "    \n",
    "    data = pd.merge(dates_ids, data[cols], on=['date', 'PID'], how='left').set_index(['date', 'PID'])\n",
    "    #print('{} - discrimination - size of perpped data: {}'.format(name, data.shape[0]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATORS = {\n",
    "   'time': None, \n",
    "   'identifier': None, \n",
    "   'affect': {'type': 'extend', 'function': process_affect}, \n",
    "   'discrimination': {'type': 'combine', 'function': process_discrimination_cmu}, \n",
    "   'social': None, \n",
    "   'workload': None, \n",
    "   'stress': None, \n",
    "   'stress_forecast': None, \n",
    "   'coping': None, \n",
    "   'regulation': None, \n",
    "   'resources': None, \n",
    "   'community': None, \n",
    "   'belonging': None, \n",
    "   'substance': None, \n",
    "   'activity': None, \n",
    "   'sleep': None, \n",
    "   'fatigue': None, \n",
    "   'overall_day': None\n",
    "# if None, the scale cannot be processed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df, col2date):\n",
    "    df['date'] = df[col2date].dt.date\n",
    "    df['date'] = df['date'].astype('datetime64[ns]')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in data\n",
    "data = {}\n",
    "grouping = {}\n",
    "for survey in surveys:\n",
    "    data[survey['name']] = pd.read_csv(survey['data_file'],\n",
    "                                       parse_dates=['start_date', 'end_date', 'recorded_date'])\n",
    "    add_date(data[survey['name']], 'start_date')\n",
    "    with open(survey['column_grouping_file'], 'r') as file_obj:\n",
    "        grouping[survey['name']] = json.load(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the fixed rows of aggregations\n",
    "start_date = study_dates['start']\n",
    "start_date = datetime.datetime(start_date['year'], \n",
    "                               start_date['month'], \n",
    "                               start_date['day'], \n",
    "                               start_date['hour'], \n",
    "                               start_date['minute'], \n",
    "                               start_date['second'])\n",
    "end_date = study_dates['end']\n",
    "end_date = datetime.datetime(end_date['year'],\n",
    "                             end_date['month'], \n",
    "                             end_date['day'], \n",
    "                             end_date['hour'], \n",
    "                             end_date['minute'], \n",
    "                             end_date['second'])\n",
    "\n",
    "dates = pd.DataFrame(pd.date_range(start=start_date, end=end_date, freq='D'), columns=['date'])\n",
    "dates['dummykey'] = 1\n",
    "\n",
    "participants = pd.DataFrame(list(range(id_range[0], id_range[1]+1)), columns=['PID'])\n",
    "pids = [200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,217,218,220,224,225,226,228,229,230,231,232,\n",
    "        233,234,235,236,237,239,240,241,242,243,244,245,246,247,248,249,250,251,252,254,255,256,257,259,260,261,262,\n",
    "        263,264,265,266,267,268,269,270,271,272,274,275,276,277,278,279,281,282,283,284,285,289,290,291,292,293,294,\n",
    "        295,296,297,298,300,301,302,303,304,305,306,307,308,309,311,312,314,315,318,319,321,322,324,325,326,327,328,\n",
    "        329,330,331,332,333,334,336,337,338,339,342,343,344,350,352,355,359,360,361,362,363,365,368,369,370,373,374,\n",
    "        376,378,379,380,381,383,386,387,388,390,391,393,394,395,396,397,398,401,402,409,410,412,413,414,415,416,417,\n",
    "        419,422,423,424,427,430,431,432,434,435,437,438,439,442,444,445,448,450,451,452,453,454,457,460,461,462,463,\n",
    "        465,466,472,473,474,476,477,479,483,484,485,487,489,491,492,497,500,506,507,510,512,513,514,515,517,518,519,\n",
    "        521,522,523,524,525,528,531,532,538,539,540,541,542,551,554,555,559,563,572,577,581,586,594,595,607,609,613,\n",
    "        615,617,629,631,644,653,655,659,662,664,665,668,669,670,671,672,673,686,693,698,703,706,711,714] # CMU phase II\n",
    "participants = pd.DataFrame(pids, columns=['PID'])\n",
    "participants['dummykey'] = 1\n",
    "\n",
    "dates_ids = pd.merge(dates, participants, on='dummykey')\n",
    "dates_ids = dates_ids.drop(columns='dummykey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizontal Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(h_aggregation_column_grouping_file, 'r') as file_obj:\n",
    "    aggregation_groups = json.load(file_obj)\n",
    "# TO-DO loop through the items in horizontal column grouping and obtain a list of columns\n",
    "# TO-DO create h_aggregated with those columns and dates_ids as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the scales that should be aggregated and call the processor function for each scale across surveys\n",
    "scales = aggregation_groups.keys()\n",
    "aggregated = {}\n",
    "for scale in scales:\n",
    "    for survey_name, survey_data in data.items():\n",
    "        if scale not in grouping[survey_name]:\n",
    "            #print('scale {} does not exist in {}'.format(scale, survey_name))\n",
    "            continue\n",
    "        if AGGREGATORS[scale] is None:\n",
    "            #print('{} - procssing {} not supported'.format(survey_name, scale))\n",
    "            continue\n",
    "        \n",
    "        aggregator_type = AGGREGATORS[scale]['type']\n",
    "        aggregator = AGGREGATORS[scale]['function']\n",
    "        prepped = aggregator(survey_name, survey_data, grouping[survey_name][scale], dates_ids)\n",
    "        \n",
    "        if aggregator_type == 'extend':\n",
    "            if scale not in aggregated:\n",
    "                aggregated[scale] = [prepped]\n",
    "            else:\n",
    "                aggregated[scale].append(prepped)\n",
    "        elif aggregator_type == 'combine':\n",
    "            if scale not in aggregated:\n",
    "                aggregated[scale] = prepped\n",
    "            else:\n",
    "                aggregated[scale] = aggregated[scale].combine_first(prepped)\n",
    "\n",
    "for scale in aggregated:\n",
    "    if AGGREGATORS[scale]['type'] == 'extend':\n",
    "        aggregated[scale] = pd.concat(aggregated[scale], axis=1)\n",
    "\n",
    "aggregated = pd.concat(list(aggregated.values()), axis=1).reset_index()\n",
    "# TO-DO insert aggregated into h_aggregated under appropriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_offset = pd.to_datetime(start_date) - pd.to_timedelta(1,unit='d')\n",
    "aggregated['day'] = (aggregated['date'] - date_offset).dt.days\n",
    "aggregated.sort_values(by=['PID', 'date'], ascending=[True, True], inplace=True)\n",
    "# NOTE sorting by PID then date ensures shifting aligns along time works on values for the same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(aggregated.columns)\n",
    "columns.remove('day')\n",
    "columns.insert(columns.index('date')+1, 'day')\n",
    "aggregated = aggregated[columns]\n",
    "aggregated.to_csv(h_aggregation_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertical Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO implementation of vertical aggregation with fixed columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">refactor the code below (e.g. to use config files or looping instead of code repetition) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = 'results/emaprep/weekly-numVal-internalID_cleaned_DISC_SUBS.csv'\n",
    "weekly_groups = '/Users/yasaman/UWEXP/script-input/emas/scale_grouping-columns-weeklyEMA.json'\n",
    "\n",
    "evening = 'results/emaprep/evening-numVal-internalID_cleaned_DISC.csv'\n",
    "evening_groups = '/Users/yasaman/UWEXP/script-input/emas/scale_grouping-columns-eveningEMA.json'\n",
    "\n",
    "midday2 = '' # TO-DO\n",
    "midday2_groups = '' # TO-DO\n",
    "\n",
    "midday1 = '' # TO-DO\n",
    "midday1_groups = '' # TO-DO\n",
    "\n",
    "morning = '' # TO-DO\n",
    "morning_groups = '' # TO-DO\n",
    "\n",
    "all_ = 'results/emaprep/all-numVal-internalID_cleaned_DISC_SUBS.csv'\n",
    "all_groups = 'results/emaprep/scale_grouping-columns-cleaned_DISC_SUBS.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = pd.read_csv(weekly)\n",
    "with open(weekly_groups, 'r') as file_obj:\n",
    "    weekly_groups = json.load(file_obj)\n",
    "    \n",
    "evening = pd.read_csv(evening)\n",
    "with open(evening_groups, 'r') as file_obj:\n",
    "    evening_groups = json.load(file_obj)\n",
    "    \n",
    "#midday2 = pd.read_csv(midday2)\n",
    "#with open(midday2_groups, 'r') as file_obj:\n",
    "#   midday2_groups = json.load(file_obj)\n",
    "\n",
    "#midday1 = pd.read_csv(midday1)\n",
    "#with open(midday1_groups, 'r') as file_obj:\n",
    "#   midday1_groups = json.load(file_obj)\n",
    "\n",
    "#morning = pd.read_csv(morning)\n",
    "#with open(morning_groups, 'r') as file_obj:\n",
    "#   morning_groups = json.load(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surveys = [weekly, evening, midday2, midday1, morning] #TO-DO\n",
    "surveys = [weekly, evening]\n",
    "concatenated = pd.concat(surveys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "groups.extend(list(weekly_groups.keys()))\n",
    "groups.extend(list(evening_groups.keys()))\n",
    "#groups.append(list(midday2_groups.keys()))\n",
    "#groups.append(list(midday1_groups.keys()))\n",
    "#groups.append(list(morning_groups.keys()))\n",
    "groups = list(set(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_groupings = {}\n",
    "for group in groups:\n",
    "    scale_group = []\n",
    "    if group in weekly_groups:\n",
    "        scale_group.extend(weekly_groups[group])\n",
    "    if group in evening_groups:\n",
    "        scale_group.extend(evening_groups[group])\n",
    "    scale_groupings[group] = list(set(scale_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(all_groups, 'w') as file_obj:\n",
    "    json.dump(scale_groupings, file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(concatenated.columns)\n",
    "columns_reordered = ['survey', 'start_date', 'recorded_date', 'pid', 'discriminated', 'any_drug', 'any_substance']\n",
    "for col in columns_reordered:\n",
    "    columns.remove(col)\n",
    "columns_reordered.extend(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = concatenated[columns_reordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated.to_csv(all_, index=False)\n",
    "# tested that the colums of concatenated is the union of the columns of the surveys\n",
    "# tested that the number of rows of concatenated is the sum of the rows of the surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO incorporate substance use data from morning surveys of the next day with evening surveys.\n",
    "#       these represent the substance use on the same day as the discrimination and are comparable with \n",
    "#       substance use data in weekly suryves.\n",
    "# TO-DO incorporate stress forcasting data from morning suryves of the next day with evening surveys.\n",
    "#       these represent the status on the day after the discimination and are comparable with stress  \n",
    "#       forcasting data in weekly surveys."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
