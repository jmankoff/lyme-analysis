{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/yasaman/UWEXP/data/cmu-phaseII-daily/'\n",
    "feature_acronyms = {\n",
    "    'activity': 'act',\n",
    "    'applications': None,\n",
    "#    'audio': 'audio', # not used for discrimination descriptive work\n",
    "#    'battery': 'batt', # not used for discrimination descriptive work\n",
    "#    'bluetooth': 'blue', # not used for discrimination descriptive work\n",
    "    'calls': 'call', \n",
    "    'locations': 'loc',\n",
    "    'screen': 'screen',\n",
    "    'sleep': 'slp',\n",
    "    'steps': 'steps',\n",
    "#    'wifi': 'wifi' # not used for discrimination descriptive work\n",
    "}\n",
    "slice_acronyms = {\n",
    "    'allday': '',\n",
    "    'morning': '_mo',\n",
    "    'afternoon': '_af',\n",
    "    'evening': '_ev',\n",
    "    'night': '_ni'\n",
    "}\n",
    "date = 'yyyy-mm-dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data = []\n",
    "sensor_columns = ['device_id', 'date']\n",
    "for feat, facr in feature_acronyms.items():\n",
    "    if facr is None:\n",
    "        continue\n",
    "    for slc, sacr in slice_acronyms.items():\n",
    "        # construct the file name\n",
    "        file_name = 'f_'+facr+','+sacr+',,_day.csv'\n",
    "        \n",
    "        # read the file in data with device_id as index\n",
    "        data = pd.read_csv(path+file_name, header=0, index_col=0)\n",
    "        \n",
    "        # construct the prefix and suffix of feature columns\n",
    "        prefix = 'f_'+facr+'_'\n",
    "        suffix = sacr+'_day_'\n",
    "        \n",
    "        cols = list(data.columns)\n",
    "        \n",
    "        # obtain feature names\n",
    "        feat_names = set([col[len(prefix):-(len(suffix)+len(date))] for col in cols if col != 'device_id'])\n",
    "        \n",
    "        # for each feature named by name\n",
    "        for name in feat_names:\n",
    "            \n",
    "            # get all columns for feature named name across different dates\n",
    "            feat_cols = [col for col in cols if col != 'device_id' if col[len(prefix):-(len(suffix)+len(date))] == name]\n",
    "            \n",
    "            # get feature data across\n",
    "            features = data[feat_cols]\n",
    "            \n",
    "            # stack feature data for all dates\n",
    "            features = features.stack().reset_index()\n",
    "            \n",
    "            # obtain dates from column names now under level_1\n",
    "            features['level_1'] = features['level_1'].apply(lambda x: x[-len(date):])\n",
    "            \n",
    "            # rename the level_1 with dates and 0 (i.e. feature data) with name of the feature\n",
    "            features = features.rename({'level_1':'date', 0:feat+'_'+name+'_'+slc}, axis='columns')\n",
    "            \n",
    "            # set device_id and date as index to facilitate concatenation\n",
    "            features = features.set_index(['device_id', 'date'])\n",
    "            \n",
    "            # add features to the list of features for concatenation\n",
    "            sensor_data.append(features)\n",
    "            \n",
    "            sensor_columns.append(feat+'_'+name+'_'+slc)\n",
    "            \n",
    "sensor_data = pd.concat(sensor_data, axis=1)\n",
    "sensor_data = sensor_data.reset_index()\n",
    "sensor_data = sensor_data[sensor_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id2pid_file = '/Users/yasaman/UWEXP/script-input/sensors/pid_device_cmu.json'\n",
    "with open(device_id2pid_file, 'r') as fileObj:\n",
    "    device_id2pid_mapping = json.load(fileObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pid (device_id, device_id2pid_mapping):\n",
    "    if device_id[-12:] in device_id2pid_mapping:\n",
    "        return device_id2pid_mapping[device_id[-12:]]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data['PID'] = sensor_data.apply(lambda x: get_pid(x['device_id'], device_id2pid_mapping), axis=1)\n",
    "#sensor_data[sensor_data['PID'].isnull()].shape[0] # should be zero if there is a PID for every device_id\n",
    "sensor_data[sensor_data['PID'].isnull()]['device_id'].unique() # device_id's with no PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(sensor_data.columns)\n",
    "columns.remove('PID')\n",
    "columns.insert(0, 'PID')\n",
    "sensor_data = sensor_data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(device_id2pid_mapping.values()) - set(sensor_data['PID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data.to_csv('/Users/yasaman/UWEXP/analysis-scripts/sensors/cmudata/results/sensors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test code follows from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'activity'\n",
    "slc = 'allday'\n",
    "df = pd.DataFrame({'device_id': [1, 2, 3], \n",
    "                   'f_act_feat1_day_2018-01-01': [11, 12, 13], \n",
    "                   'f_act_feat1_day_2018-01-02': [-11, -12, -13],\n",
    "                   'f_act_feat2_day_2018-01-01': [101, 102, 103], \n",
    "                   'f_act_feat2_day_2018-01-02': [-101, -102, -103]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'f_'+feature_acronyms[feat]+'_'\n",
    "suffix = slice_acronyms[slc]+'_day_'\n",
    "date = 'yyyy-mm-dd'\n",
    "cols = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = set([col[len(prefix):-(len(suffix)+len(date))] for col in cols if col != 'device_id'])\n",
    "#name_mapping = {col: col[len(prefix):-(len(suffix)+len(date))]+col[-(len(date)+1):] for col in cols if col != 'device_id'}\n",
    "#df = df.rename(name_mapping, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.set_index('device_id')\n",
    "new_data = []\n",
    "columns = ['device_id', 'date']\n",
    "for name in feat_names:\n",
    "    # get all columns for feature named name across different dates\n",
    "    feat_cols = [col for col in cols if col != 'device_id' if col[len(prefix):-(len(suffix)+len(date))] == name]\n",
    "    \n",
    "    # get feature data across\n",
    "    features = df2[feat_cols]\n",
    "    \n",
    "    # stack feature data for all dates\n",
    "    features = features.stack().reset_index()\n",
    "    \n",
    "    # obtain dates from column names now under level_1\n",
    "    features['level_1'] = features['level_1'].apply(lambda x: x[-len(date):])\n",
    "    \n",
    "    # rename the level_1 with dates and 0 (i.e. feature data) with name of the feature\n",
    "    features = features.rename({'level_1':'date', 0:feat+'_'+name+'_'+slc}, axis='columns')\n",
    "    \n",
    "    # set device_id and date as index to facilitate concatenation\n",
    "    features = features.set_index(['device_id', 'date'])\n",
    "    \n",
    "    # add features to the list of features for concatenation\n",
    "    new_data.append(features)\n",
    "    \n",
    "    columns.append(feat+'_'+name+'_'+slc)\n",
    "\n",
    "new_data = pd.concat(new_data, axis=1)\n",
    "new_data = new_data.reset_index()\n",
    "new_data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.set_index('device_id')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[['f_act_feat1_day_2018-01-01', 'f_act_feat1_day_2018-01-02']].stack().reset_index()\n",
    "df2['level_1'] = df2['level_1'].apply(lambda x: x[-len(date):])\n",
    "df2\n",
    "# rename level_1 to date and 0 to feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'feat1'\n",
    "[col for col in cols if col != 'device_id' if col[len(prefix):-(len(suffix)+len(date))] == name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_did_cmu_file = '/Users/yasaman/Downloads/PID-device.csv'\n",
    "pid_did_cmu = pd.read_csv(pid_did_cmu_file, dtype={'ID': 'int32',\n",
    "                                                   'Andrew ID': 'str',\n",
    "                                                   'AWARE Device ID': 'str',\n",
    "                                                   'New Device ID': 'str', \n",
    "                                                   'New Device ID #2': 'str', \n",
    "                                                   'New Device ID #3': 'str', \n",
    "                                                   'Phone #': 'str',\n",
    "                                                   'Cell Phone Provider': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_id(row):\n",
    "    NaN = row.isnull()\n",
    "    if NaN['New Device ID #3'] == False:\n",
    "        #print('returning3: {}'.format(row['New Device ID #3']))\n",
    "        return row['New Device ID #3']\n",
    "    if NaN['New Device ID #2'] == False:\n",
    "        #print('returning2: {}'.format(row['New Device ID #2']))\n",
    "        return row['New Device ID #2']\n",
    "    if NaN['New Device ID'] == False:\n",
    "        #print('returning1: {}'.format(row['New Device ID']))\n",
    "        return row['New Device ID']\n",
    "    if NaN['AWARE Device ID'] == False:\n",
    "        #print('returning0: {}'.format(row['AWARE Device ID']))\n",
    "        return row['AWARE Device ID']\n",
    "    #print('returning None')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_did_cmu['device_id'] = pid_did_cmu.apply(lambda x: get_latest_id(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_did_cmu.to_csv('/Users/yasaman/Downloads/pid_device_cmu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
